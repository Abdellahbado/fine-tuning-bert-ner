{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a83a35b363e4ff8b56e663266a1605f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['tokens', 'labels'],\n",
      "        num_rows: 226\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "file_path = \"datasets/ner-large-dataset-train.json\"\n",
    "\n",
    "# Load the dataset from the file\n",
    "dataset = load_dataset('json', data_files=file_path)\n",
    "\n",
    "# Print the dataset to verify\n",
    "print(dataset)\n",
    "data = dataset[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_labels = set(label for example in data for label in example[\"labels\"])\n",
    "label2id = {label: id for id, label in enumerate(unique_labels)}\n",
    "id2label = {id: label for label, id in label2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'B-Experience_Level',\n",
       " 1: 'B-Job_role',\n",
       " 2: 'B-Skill',\n",
       " 3: 'O',\n",
       " 4: 'B-Job_Role',\n",
       " 5: 'B-Domain'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'B-Experience_Level': 0,\n",
       " 'B-Job_role': 1,\n",
       " 'B-Skill': 2,\n",
       " 'O': 3,\n",
       " 'B-Job_Role': 4,\n",
       " 'B-Domain': 5}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = dataset['train']['labels']\n",
    "\n",
    "encoded_labels = []\n",
    "for label_sequence in labels:\n",
    "    encoded_sequence = [label2id[label] for label in label_sequence]\n",
    "    encoded_labels.append(encoded_sequence)\n",
    "\n",
    "encoded_dataset = dataset['train'].add_column('encoded_labels', encoded_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens:  ['We', 'are', 'seeking', 'an', 'experienced', 'Senior', 'Software', 'Engineer', 'with', 'expertise', 'in', 'Python', ',', 'Django', ',', 'and', 'RESTful', 'API', 'development', 'to', 'join', 'our', 'backend', 'team', '.', 'The', 'ideal', 'candidate', 'should', 'have', '5+', 'years', 'of', 'experience', 'in', 'building', 'scalable', 'and', 'high-performance', 'web', 'applications', 'in', 'the', 'tech', 'industry', '.']\n",
      "labels:  ['O', 'O', 'O', 'O', 'O', 'B-Experience_Level', 'B-Experience_Level', 'B-Job_Role', 'O', 'O', 'O', 'O', 'B-Skill', 'O', 'B-Skill', 'O', 'B-Skill', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Domain', 'O']\n",
      "encoded_labels:  [3, 3, 3, 3, 3, 0, 0, 4, 3, 3, 3, 3, 2, 3, 2, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 5, 3]\n",
      "tokens:  ['A', 'leading', 'healthcare', 'company', 'is', 'looking', 'for', 'a', 'Mid-level', 'Data', 'Scientist', 'proficient', 'in', 'machine', 'learning', 'algorithms,', 'data', 'mining,', 'and', 'Python.', 'Experience', 'with', 'TensorFlow', 'or', 'PyTorch', 'is', 'a', 'plus.', 'Responsibilities', 'include', 'building', 'predictive', 'models,', 'conducting', 'data', 'analysis,', 'and', 'collaborating', 'with', 'cross-functional', 'teams.']\n",
      "labels:  ['O', 'O', 'B-Domain', 'O', 'O', 'O', 'O', 'B-Experience_Level', 'B-Job_Role', 'O', 'O', 'B-Skill', 'O', 'B-Skill', 'B-Skill', 'O', 'B-Skill', 'O', 'O', 'O', 'B-Skill', 'O', 'B-Skill', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "encoded_labels:  [3, 3, 5, 3, 3, 3, 3, 0, 4, 3, 3, 2, 3, 2, 2, 3, 2, 3, 3, 3, 2, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n",
      "tokens:  ['We', 'are', 'hiring', 'an', 'Entry-level', 'Frontend', 'Developer', 'with', 'strong', 'skills', 'in', 'JavaScript,', 'React,', 'and', 'CSS', 'to', 'join', 'our', 'e-commerce', 'team.', 'The', 'ideal', 'candidate', 'should', 'have', 'a', 'passion', 'for', 'building', 'responsive', 'and', 'user-friendly', 'web', 'applications.', 'Knowledge', 'of', 'Redux', 'and', 'experience', 'with', 'Agile', 'methodologies', 'is', 'preferred.']\n",
      "labels:  ['O', 'O', 'O', 'B-Experience_Level', 'B-Job_Role', 'O', 'O', 'O', 'O', 'B-Skill', 'O', 'B-Skill', 'O', 'B-Skill', 'O', 'O', 'O', 'B-Domain', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Skill', 'O', 'O', 'O', 'O']\n",
      "encoded_labels:  [3, 3, 3, 0, 4, 3, 3, 3, 3, 2, 3, 2, 3, 2, 3, 3, 3, 5, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3]\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print('tokens: ', encoded_dataset[i]['tokens'])\n",
    "    print('labels: ', encoded_dataset[i]['labels'])\n",
    "    print('encoded_labels: ', encoded_dataset[i]['encoded_labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-base-NER\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89a5ab696e5d49eeae566a7e89e0f663",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/226 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "def tokenize_and_align_tags(records):\n",
    "    # Tokenize the input words. This will break words into subtokens if necessary.\n",
    "    # For instance, \"ChatGPT\" might become [\"Chat\", \"##G\", \"##PT\"].\n",
    "    tokenized_results = tokenizer(records[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "\n",
    "    input_tags_list = []\n",
    "\n",
    "    # Iterate through each set of tags in the records.\n",
    "    for i, given_tags in enumerate(records[\"encoded_labels\"]):\n",
    "        # Get the word IDs corresponding to each token. This tells us to which original word each token corresponds.\n",
    "        word_ids = tokenized_results.word_ids(batch_index=i)\n",
    "\n",
    "        previous_word_id = None\n",
    "        input_tags = []\n",
    "\n",
    "        # For each token, determine which tag it should get.\n",
    "        for wid in word_ids:\n",
    "            # If the token does not correspond to any word (e.g., it's a special token), set its tag to -100.\n",
    "            if wid is None:\n",
    "                input_tags.append(-100)\n",
    "            # If the token corresponds to a new word, use the tag for that word.\n",
    "            elif wid != previous_word_id:\n",
    "                if wid < len(given_tags):\n",
    "                    input_tags.append(given_tags[wid])\n",
    "                else:\n",
    "                    input_tags.append(-100)\n",
    "            # If the token is a subtoken (i.e., part of a word we've already tagged), set its tag to -100.\n",
    "            else:\n",
    "                input_tags.append(-100)\n",
    "            previous_word_id = wid\n",
    "\n",
    "        input_tags_list.append(input_tags)\n",
    "\n",
    "    # Add the assigned tags to the tokenized results.\n",
    "    # In the Hugging Face Transformers library, a model recognizes the labels parameter\n",
    "    # for computing losses along with logits (predictions)\n",
    "    tokenized_results[\"labels\"] = input_tags_list\n",
    "\n",
    "    return tokenized_results\n",
    "\n",
    "tokenized_encoded_dataset = encoded_dataset.map(tokenize_and_align_tags, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "2024-04-20 15:04:29.219223: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-04-20 15:04:29.501490: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-04-20 15:04:29.501624: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-04-20 15:04:29.539875: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-04-20 15:04:29.650705: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-20 15:04:31.225527: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at dslim/bert-base-NER and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([9]) in the checkpoint and torch.Size([6]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([9, 768]) in the checkpoint and torch.Size([6, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    \"dslim/bert-base-NER\", num_labels=len(id2label), id2label=id2label, label2id=label2id, ignore_mismatched_sizes=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"my_finetuned_ner_model\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abdallah/anaconda3/lib/python3.9/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94cec602d84444e5a871e13303830115",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/87 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 212.9294, 'train_samples_per_second': 3.184, 'train_steps_per_second': 0.409, 'train_loss': 0.6573095650508486, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=87, training_loss=0.6573095650508486, metrics={'train_runtime': 212.9294, 'train_samples_per_second': 3.184, 'train_steps_per_second': 0.409, 'train_loss': 0.6573095650508486, 'epoch': 3.0})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_encoded_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/home/abdallah/Documents/new AI/Transformers/NER-fine-tuning/my_finetuned_ner_model/tokenizer_config.json',\n",
       " '/home/abdallah/Documents/new AI/Transformers/NER-fine-tuning/my_finetuned_ner_model/special_tokens_map.json',\n",
       " '/home/abdallah/Documents/new AI/Transformers/NER-fine-tuning/my_finetuned_ner_model/vocab.txt',\n",
       " '/home/abdallah/Documents/new AI/Transformers/NER-fine-tuning/my_finetuned_ner_model/added_tokens.json',\n",
       " '/home/abdallah/Documents/new AI/Transformers/NER-fine-tuning/my_finetuned_ner_model/tokenizer.json')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path=\"/home/abdallah/Documents/new AI/Transformers/NER-fine-tuning/my_finetuned_ner_model\"\n",
    "\n",
    "\n",
    "model.save_pretrained(model_path)\n",
    "\n",
    "tokenizer.save_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertConfig\n",
    "\n",
    "# Load pre-trained BERT model configuration\n",
    "pretrained_config = BertConfig.from_pretrained(\"dslim/bert-base-NER\")\n",
    "\n",
    "# Modify configuration parameters for fine-tuning\n",
    "pretrained_config.num_labels = len(id2label)  # Set the number of labels\n",
    "pretrained_config.id2label = id2label  # Set the id to label mapping\n",
    "pretrained_config.label2id = label2id  # Set the label to id mapping\n",
    "pretrained_config.ignore_mismatched_sizes = True  # Ignore mismatched sizes\n",
    "\n",
    "# Serialize modified configuration object to JSON string\n",
    "config_json = pretrained_config.to_json_string()\n",
    "\n",
    "# Save JSON string to config.json file\n",
    "with open(\"config.json\", \"w\") as config_file:\n",
    "    config_file.write(config_json)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, AutoModelForTokenClassification, AutoTokenizer\n",
    "\n",
    "\n",
    "# Load the fine-tuned model and tokenizer\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# Create a pipeline for named entity recognition (NER)\n",
    "ner_pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity: Junior, Type: B-Experience_Level\n",
      "Entity: Data, Type: B-Job_Role\n",
      "Entity: Scientist, Type: B-Job_Role\n",
      "Entity: machine, Type: B-Skill\n",
      "Entity: learning, Type: B-Skill\n",
      "Entity: Python, Type: B-Skill\n",
      "Entity: rub, Type: B-Skill\n"
     ]
    }
   ],
   "source": [
    "# Example input text\n",
    "tokens = \"Junior Data Scientist with expertise in machine learning and Python and ruby.\"\n",
    "results = ner_pipeline(tokens)\n",
    "\n",
    "# Display the results\n",
    "for result in results:\n",
    "    print(f\"Entity: {result['word']}, Type: {result['entity']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
